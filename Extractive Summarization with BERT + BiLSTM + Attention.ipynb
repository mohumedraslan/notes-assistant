{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NLP Project: Extractive Summarization with BERT + BiLSTM + Attention\n",
                "\n",
                "## Project Goal\n",
                "- Build an extractive summarization model for messy study notes, lecture transcripts, and textbook excerpts.\n",
                "- **Core Model**: BERT (Freeze) + BiLSTM + Attention.\n",
                "- **Datasets**: Webis-TLDR-17 + WikiHow.\n",
                "- **Improvements**: \n",
                "    1. Pre-trained BERT Embeddings (Better Semantics)\n",
                "    2. Trigram Blocking (Redundancy Removal)\n",
                "    3. Validation Split & Scheduler (Better Training)\n",
                "\n",
                "## Methodology\n",
                "1. **Data Loading**: Load and combine datasets.\n",
                "2. **Preprocessing**: Convert abstractive summaries to extractive labels (Oracle extraction).\n",
                "3. **Tokenization**: Use BERT tokenizer.\n",
                "4. **Model**: BERT -> BiLSTM -> Attention -> Sentence Importance Score.\n",
                "5. **Training**: Binary Cross Entropy Loss with Validation Checkpointing."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies\n",
                "Note: For GPU support on Windows/Linux, we specify the CUDA version for PyTorch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: datasets in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.4.1)\n",
                        "Requirement already satisfied: transformers in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.52.4)\n",
                        "Requirement already satisfied: rouge-score in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.1.2)\n",
                        "Requirement already satisfied: nltk in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.2)\n",
                        "Requirement already satisfied: filelock in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.18.0)\n",
                        "Requirement already satisfied: numpy>=1.17 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.2.6)\n",
                        "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (22.0.0)\n",
                        "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.4.0)\n",
                        "Requirement already satisfied: pandas in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.3.1)\n",
                        "Requirement already satisfied: requests>=2.32.2 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.32.4)\n",
                        "Requirement already satisfied: httpx<1.0.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.28.1)\n",
                        "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.67.1)\n",
                        "Requirement already satisfied: xxhash in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.6.0)\n",
                        "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.18)\n",
                        "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.5.1)\n",
                        "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.35.3)\n",
                        "Requirement already satisfied: packaging in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (25.0)\n",
                        "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0.2)\n",
                        "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
                        "Requirement already satisfied: anyio in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
                        "Requirement already satisfied: certifi in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.6.15)\n",
                        "Requirement already satisfied: httpcore==1.* in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
                        "Requirement already satisfied: idna in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
                        "Requirement already satisfied: h11>=0.16 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
                        "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.14.0)\n",
                        "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2024.11.6)\n",
                        "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.21.2)\n",
                        "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.5.3)\n",
                        "Requirement already satisfied: absl-py in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge-score) (2.3.1)\n",
                        "Requirement already satisfied: six>=1.14.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rouge-score) (1.17.0)\n",
                        "Requirement already satisfied: click in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.2.1)\n",
                        "Requirement already satisfied: joblib in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.5.1)\n",
                        "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
                        "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
                        "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
                        "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
                        "Requirement already satisfied: colorama in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
                        "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n",
                        "Requirement already satisfied: sniffio>=1.1 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[notice] A new release of pip is available: 25.2 -> 25.3\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
                        "Requirement already satisfied: torch in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cu121)\n",
                        "Requirement already satisfied: torchvision in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.20.1+cu121)\n",
                        "Requirement already satisfied: torchaudio in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1+cu121)\n",
                        "Requirement already satisfied: filelock in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.18.0)\n",
                        "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.14.0)\n",
                        "Requirement already satisfied: networkx in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.4.2)\n",
                        "Requirement already satisfied: jinja2 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.1.6)\n",
                        "Requirement already satisfied: fsspec in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (2025.5.1)\n",
                        "Requirement already satisfied: sympy==1.13.1 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.13.1)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
                        "Requirement already satisfied: numpy in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.2.6)\n",
                        "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (11.2.1)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\moras\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[notice] A new release of pip is available: 25.2 -> 25.3\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                }
            ],
            "source": [
                "%pip install datasets transformers rouge-score nltk\n",
                "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Imports & Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n",
                        "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader, random_split\n",
                "from datasets import load_dataset, concatenate_datasets\n",
                "from transformers import BertTokenizer, BertModel\n",
                "from rouge_score import rouge_scorer\n",
                "import nltk\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "import re\n",
                "import os\n",
                "\n",
                "# Download NLTK data\n",
                "try:\n",
                "    nltk.data.find('tokenizers/punkt')\n",
                "except LookupError:\n",
                "    nltk.download('punkt')\n",
                "    nltk.download('punkt_tab')\n",
                "\n",
                "# Set device\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "if device.type == 'cuda':\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Datasets\n",
                "We load Webis-TLDR-17 and WikiHow. We ensure both have uniform columns: `text` and `summary`.\n",
                "Note: If WikiHow is unavailable, we fall back to CNN/DailyMail."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading Webis-TLDR-17...\n",
                        "Webis dataset load failed: Dataset scripts are no longer supported, but found tldr-17.py. Proceeding without it.\n",
                        "Loading WikiHow...\n",
                        "WikiHow load failed: Dataset 'wikihow' doesn't exist on the Hub or cannot be accessed.. Switching to CNN/DailyMail as fallback.\n",
                        "Combined Dataset Size: 6000\n"
                    ]
                }
            ],
            "source": [
                "# Configuration\n",
                "MAX_SAMPLES = 6000 # Increased slightly for better training\n",
                "\n",
                "# 1. Load Webis-TLDR-17\n",
                "print(\"Loading Webis-TLDR-17...\")\n",
                "try:\n",
                "    dataset_webis = load_dataset(\"webis/tldr-17\", split=\"train[:5%]\")\n",
                "except Exception as e:\n",
                "    print(f\"Webis dataset load failed: {e}. Proceeding without it.\")\n",
                "    dataset_webis = None\n",
                "\n",
                "# 2. Load WikiHow (with Fallback)\n",
                "print(\"Loading WikiHow...\")\n",
                "try:\n",
                "    dataset_wikihow = load_dataset(\"wikihow\", \"all\", split=\"train[:10%]\")\n",
                "except Exception as e:\n",
                "    print(f\"WikiHow load failed: {e}. Switching to CNN/DailyMail as fallback.\")\n",
                "    dataset_wikihow = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:10%]\")\n",
                "\n",
                "# 3. Unify Columns\n",
                "def unify_columns(dataset, text_col, summary_col):\n",
                "    cols_to_keep = [text_col, summary_col]\n",
                "    dataset = dataset.remove_columns([c for c in dataset.column_names if c not in cols_to_keep])\n",
                "    dataset = dataset.rename_column(text_col, \"text\")\n",
                "    dataset = dataset.rename_column(summary_col, \"summary\")\n",
                "    return dataset\n",
                "\n",
                "if dataset_webis:\n",
                "    if 'content' in dataset_webis.column_names:\n",
                "        dataset_webis = unify_columns(dataset_webis, 'content', 'summary')\n",
                "    elif 'body' in dataset_webis.column_names:\n",
                "        dataset_webis = unify_columns(dataset_webis, 'body', 'summary')\n",
                "\n",
                "# Check for WikiHow or Fallback columns\n",
                "if 'headline' in dataset_wikihow.column_names: \n",
                "    dataset_wikihow = unify_columns(dataset_wikihow, 'text', 'headline')\n",
                "elif 'highlights' in dataset_wikihow.column_names:\n",
                "    dataset_wikihow = unify_columns(dataset_wikihow, 'article', 'highlights')\n",
                "\n",
                "# 4. Concatenate\n",
                "if dataset_webis:\n",
                "    combined_dataset = concatenate_datasets([dataset_webis, dataset_wikihow])\n",
                "else:\n",
                "    combined_dataset = dataset_wikihow\n",
                "\n",
                "# Shuffle and limit\n",
                "combined_dataset = combined_dataset.shuffle(seed=42).select(range(min(len(combined_dataset), MAX_SAMPLES)))\n",
                "\n",
                "print(f\"Combined Dataset Size: {len(combined_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Preprocessing: Oracle Label Generation\n",
                "Extract sentence-level labels (0/1) based on ROUGE overlap with the abstractive summary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
                "\n",
                "def greedy_extraction(text, summary):\n",
                "    # Split sentences\n",
                "    sentences = nltk.sent_tokenize(text)\n",
                "    \n",
                "    # Clean/lowercase\n",
                "    sentences = [s.strip() for s in sentences if len(s.split()) > 3] \n",
                "    \n",
                "    if not sentences:\n",
                "        return [], []\n",
                "\n",
                "    labels = []\n",
                "    for sent in sentences:\n",
                "        scores = scorer.score(summary, sent)\n",
                "        r1_rec = scores['rouge1'].recall\n",
                "        rL_rec = scores['rougeL'].recall\n",
                "        \n",
                "        # Label 1 if sentence overlaps significantly with summary\n",
                "        if r1_rec > 0.15 or rL_rec > 0.15: # Slightly stricter threshold for better quality\n",
                "            labels.append(1)\n",
                "        else:\n",
                "            labels.append(0)\n",
                "            \n",
                "    return sentences, labels"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Dataset Class & Tokenizer\n",
                "We use `BertTokenizer`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Preprocessing data...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 6000/6000 [01:52<00:00, 53.53it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processed 178970 sentences.\n",
                        "Train samples: 161073, Val samples: 17897\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
                "\n",
                "class ExtractiveDataset(Dataset):\n",
                "    def __init__(self, hf_dataset, tokenizer, max_len=64):\n",
                "        self.samples = []\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_len = max_len\n",
                "        \n",
                "        print(\"Preprocessing data...\")\n",
                "        for item in tqdm(hf_dataset):\n",
                "            text = item['text']\n",
                "            summary = item['summary']\n",
                "            \n",
                "            sents, labels = greedy_extraction(text, summary)\n",
                "            for sent, label in zip(sents, labels):\n",
                "                self.samples.append((sent, label))\n",
                "            \n",
                "        print(f\"Processed {len(self.samples)} sentences.\")\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        sentence, label = self.samples[idx]\n",
                "        \n",
                "        encoding = self.tokenizer(\n",
                "            sentence,\n",
                "            truncation=True,\n",
                "            padding='max_length',\n",
                "            max_length=self.max_len,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        \n",
                "        return {\n",
                "            'input_ids': encoding['input_ids'].flatten(),\n",
                "            'attention_mask': encoding['attention_mask'].flatten(),\n",
                "            'labels': torch.tensor(label, dtype=torch.float)\n",
                "        }\n",
                "\n",
                "# Create Dataset and Split\n",
                "full_dataset = ExtractiveDataset(combined_dataset, tokenizer, max_len=64)\n",
                "\n",
                "# 90/10 Train/Val Split\n",
                "train_size = int(0.9 * len(full_dataset))\n",
                "val_size = len(full_dataset) - train_size\n",
                "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
                "\n",
                "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Improved Model: BERT + BiLSTM + Attention\n",
                "We replace the basic embedding layer with **BERT** to capture deep context. We freeze BERT to save compute/memory, treating it as a feature extractor."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BiLSTMBertAttention(nn.Module):\n",
                "    def __init__(self, hidden_dim):\n",
                "        super(BiLSTMBertAttention, self).__init__()\n",
                "        # 1. Load Pre-trained BERT\n",
                "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
                "        \n",
                "        # Freeze BERT parameters\n",
                "        for param in self.bert.parameters():\n",
                "            param.requires_grad = False\n",
                "            \n",
                "        bert_dim = 768 # bert-base output dimension\n",
                "        \n",
                "        # 2. BiLSTM\n",
                "        self.lstm = nn.LSTM(bert_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
                "        \n",
                "        # 3. Attention Weights\n",
                "        self.W_w = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
                "        self.u_w = nn.Linear(hidden_dim * 2, 1, bias=False)\n",
                "        \n",
                "        # 4. Classification Head\n",
                "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
                "        self.sigmoid = nn.Sigmoid()\n",
                "\n",
                "    def forward(self, input_ids, attention_mask):\n",
                "        # Get BERT features | [Batch, SeqLen, 768]\n",
                "        with torch.no_grad():\n",
                "            bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
                "            embeddings = bert_outputs.last_hidden_state\n",
                "        \n",
                "        # LSTM | [Batch, SeqLen, Hidden*2]\n",
                "        lstm_out, _ = self.lstm(embeddings)\n",
                "        \n",
                "        # Attention\n",
                "        # u = tanh(W * h)\n",
                "        u = torch.tanh(self.W_w(lstm_out))\n",
                "        # score = u * u_w\n",
                "        scores = self.u_w(u) # [Batch, SeqLen, 1]\n",
                "        \n",
                "        # Masking\n",
                "        mask = attention_mask.unsqueeze(-1)\n",
                "        scores = scores.masked_fill(mask == 0, -1e9)\n",
                "            \n",
                "        alpha = torch.softmax(scores, dim=1) # [Batch, SeqLen, 1]\n",
                "        \n",
                "        # Sentence Vector\n",
                "        sentence_vector = torch.sum(lstm_out * alpha, dim=1) # [Batch, Hidden*2]\n",
                "        \n",
                "        # Prediction\n",
                "        prediction = self.sigmoid(self.fc(sentence_vector))\n",
                "        \n",
                "        return prediction.squeeze()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Improved Training Loop\n",
                "- Validation Loss tracking\n",
                "- ReduceLROnPlateau Scheduler\n",
                "- Save Best Model Checkpoint"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1 Train: 100%|██████████| 5034/5034 [06:46<00:00, 12.39it/s, loss=0.464]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1 | Train Loss: 0.4190 | Val Loss: 0.4125\n",
                        "  -> Saved Best Model!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2 Train: 100%|██████████| 5034/5034 [06:56<00:00, 12.09it/s, loss=0.263]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2 | Train Loss: 0.4028 | Val Loss: 0.3974\n",
                        "  -> Saved Best Model!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3 Train: 100%|██████████| 5034/5034 [07:15<00:00, 11.56it/s, loss=0.556] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3 | Train Loss: 0.3964 | Val Loss: 0.3971\n",
                        "  -> Saved Best Model!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4 Train: 100%|██████████| 5034/5034 [07:21<00:00, 11.39it/s, loss=0.342]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4 | Train Loss: 0.3896 | Val Loss: 0.3971\n",
                        "  -> Saved Best Model!\n"
                    ]
                }
            ],
            "source": [
                "model = BiLSTMBertAttention(hidden_dim=128).to(device)\n",
                "\n",
                "criterion = nn.BCELoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=1e-4) # Lower LR for fine-tuning stability\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
                "\n",
                "EPOCHS = 4\n",
                "best_val_loss = float('inf')\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    # --- Training ---\n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    progress = tqdm(train_loader, desc=f\"Epoch {epoch+1} Train\")\n",
                "    \n",
                "    for batch in progress:\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        mask = batch['attention_mask'].to(device)\n",
                "        labels = batch['labels'].to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(input_ids, mask)\n",
                "        \n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        train_loss += loss.item()\n",
                "        progress.set_postfix(loss=loss.item())\n",
                "        \n",
                "    avg_train_loss = train_loss / len(train_loader)\n",
                "    \n",
                "    # --- Validation ---\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    with torch.no_grad():\n",
                "        for batch in val_loader:\n",
                "            input_ids = batch['input_ids'].to(device)\n",
                "            mask = batch['attention_mask'].to(device)\n",
                "            labels = batch['labels'].to(device)\n",
                "            \n",
                "            outputs = model(input_ids, mask)\n",
                "            loss = criterion(outputs, labels)\n",
                "            val_loss += loss.item()\n",
                "            \n",
                "    avg_val_loss = val_loss / len(val_loader)\n",
                "    \n",
                "    # Scheduler Step\n",
                "    scheduler.step(avg_val_loss)\n",
                "    \n",
                "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
                "    \n",
                "    # Checkpointing\n",
                "    if avg_val_loss < best_val_loss:\n",
                "        best_val_loss = avg_val_loss\n",
                "        torch.save(model.state_dict(), \"best_model_bert.pth\")\n",
                "        print(\"  -> Saved Best Model!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Trigram Blocking Evaluation\n",
                "We implement Trigram Blocking to reduce repetition in the final summary."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\moras\\AppData\\Local\\Temp\\ipykernel_39516\\3913527863.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
                        "  model.load_state_dict(torch.load(\"best_model_bert.pth\"))\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Reference: A small amount of radioactive gas escaped from a steam generator, the NRC says .\n",
                        "The leak does not pose any threat to human health, an NRC spokesman says .\n",
                        "Operators shut down the No. 3 reactor at California's San Onofre plant as a result .\n",
                        "Prediction: (CNN) -- A small amount of radioactive gas escaped from a steam generator at Southern California's San Onofre nuclear power plant during a water leak, but there was no threat to public health, federal regulators said Wednesday. The water leak occurred in the thousands of tubes that carry heated water from the reactor core through the steam generator, a 65-foot-tall, 640-ton piece of equipment that boils water used to drive the unit's turbines. Though leaking tubes periodically occur in older units, Dricks said, Southern California Edison replaced the steam generators at San Onofre between 2009 and 2011.\n",
                        "------------------------------\n",
                        "ROUGE-1: 0.37241379310344824\n",
                        "ROUGE-L: 0.27586206896551724\n"
                    ]
                }
            ],
            "source": [
                "def get_trigrams(sentence):\n",
                "    tokens = sentence.lower().split()\n",
                "    if len(tokens) < 3:\n",
                "        return set()\n",
                "    return set(tuple(tokens[i:i+3]) for i in range(len(tokens)-2))\n",
                "\n",
                "def has_trigram_overlap(sent, selected_sents):\n",
                "    # Check if 'sent' shares any trigram with already selected sentences\n",
                "    new_trigrams = get_trigrams(sent)\n",
                "    if not new_trigrams: return False\n",
                "    \n",
                "    for existing in selected_sents:\n",
                "        existing_trigrams = get_trigrams(existing)\n",
                "        if not new_trigrams.isdisjoint(existing_trigrams):\n",
                "            return True\n",
                "    return False\n",
                "\n",
                "def summarize_improved(text, model, tokenizer, device, top_k=3):\n",
                "    model.eval()\n",
                "    sentences = nltk.sent_tokenize(text)\n",
                "    cleaned_sentences = [s.strip() for s in sentences if len(s.split()) > 3]\n",
                "    \n",
                "    if not cleaned_sentences: return \"No content\"\n",
                "    \n",
                "    inputs = tokenizer(\n",
                "        cleaned_sentences,\n",
                "        return_tensors='pt',\n",
                "        truncation=True,\n",
                "        padding='max_length',\n",
                "        max_length=64\n",
                "    )\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        input_ids = inputs['input_ids'].to(device)\n",
                "        mask = inputs['attention_mask'].to(device)\n",
                "        scores = model(input_ids, mask)\n",
                "        \n",
                "    if scores.ndim == 0: scores = scores.unsqueeze(0)\n",
                "    scores = scores.cpu().numpy()\n",
                "    \n",
                "    sorted_indices = scores.argsort()[::-1]\n",
                "    \n",
                "    selected_sents = []\n",
                "    for idx in sorted_indices:\n",
                "        candidate = cleaned_sentences[idx]\n",
                "        \n",
                "        # Trigram Blocking\n",
                "        if not has_trigram_overlap(candidate, selected_sents):\n",
                "            selected_sents.append(candidate)\n",
                "            \n",
                "        if len(selected_sents) >= top_k:\n",
                "            break\n",
                "            \n",
                "    return \" \".join(selected_sents)\n",
                "\n",
                "# Load Best Model for Inference\n",
                "model.load_state_dict(torch.load(\"best_model_bert.pth\"))\n",
                "\n",
                "# Sample Test\n",
                "test_idx = 0\n",
                "txt = combined_dataset[test_idx]['text']\n",
                "ref = combined_dataset[test_idx]['summary']\n",
                "pred = summarize_improved(txt, model, tokenizer, device)\n",
                "\n",
                "print(\"Reference:\", ref)\n",
                "print(\"Prediction:\", pred)\n",
                "print(\"-\"*30)\n",
                "\n",
                "# Quick ROUGE eval\n",
                "eval_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
                "scores = eval_scorer.score(ref, pred)\n",
                "print(\"ROUGE-1:\", scores['rouge1'].fmeasure)\n",
                "print(\"ROUGE-L:\", scores['rougeL'].fmeasure)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
